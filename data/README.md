# ğŸ“Š Data Directory

This directory contains all data used in the bike availability prediction project.

---

## ğŸ“ Directory Structure

```
data/
â”œâ”€â”€ raw/           # Original, immutable data
â””â”€â”€ processed/     # Cleaned and transformed data
```

---

## ğŸ—‚ï¸ Data Organization

### `raw/` - Raw Data

**Purpose**: Store original, unmodified datasets exactly as downloaded

**Guidelines**:
- âœ… Keep data in original format (CSV, JSON, etc.)
- âœ… Never modify files in this folder
- âœ… Include data source documentation
- âœ… Add date stamps to filenames

**Naming Convention**:
```
{source}_{description}_{YYYY-MM-DD}.{ext}

Examples:
â”œâ”€â”€ amsterdam_bike_api_2024-01-15.json
â”œâ”€â”€ knmi_weather_2024-01-15.csv
â”œâ”€â”€ openchargemap_stations_2024-01-15.json
â””â”€â”€ README.md (describe each file)
```

**See**: [raw/README.md](raw/README.md) for detailed organization

---

### `processed/` - Processed Data

**Purpose**: Store cleaned, merged, and feature-engineered datasets

**Guidelines**:
- âœ… Document all transformations
- âœ… Include version numbers
- âœ… Save in efficient formats (parquet, feather)
- âœ… Keep processing scripts traceable

**Naming Convention**:
```
{stage}_{description}_v{version}.{ext}

Examples:
â”œâ”€â”€ cleaned_bike_weather_v1.parquet
â”œâ”€â”€ featured_bike_availability_v2.parquet
â””â”€â”€ README.md (describe transformations)
```

**See**: [processed/README.md](processed/README.md) for detailed organization

---

## ğŸŒ Data Sources

For this project, you'll work with open data from:

| Source | Data Type | Module |
|--------|-----------|--------|
| Amsterdam Open Data | Bike availability | Module 02 |
| KNMI | Weather data | Module 02 |
| OpenChargeMap | Charging stations | Module 02 |
| Custom APIs | Time-series data | Module 02 |

**Complete list**: See [docs/open_data_sources.md](../docs/open_data_sources.md)

---

## ğŸ”’ Data Security & Privacy

### Do NOT Commit

- âŒ Large datasets (> 100MB)
- âŒ Personal/sensitive data
- âŒ API keys or credentials
- âŒ Raw data downloads (unless small samples)

### What to Use Instead

- âœ… `.gitignore` entries for data files
- âœ… Sample datasets (< 10MB)
- âœ… Data acquisition scripts
- âœ… Documentation of data sources

---

## ğŸ“¥ Getting Data

### Option 1: Download from Sources

```python
# See Module 02 notebooks for API examples
import requests

# Example: Amsterdam Bike API
url = "https://api.amsterdam.nl/bikes"
response = requests.get(url)
data = response.json()
```

### Option 2: Generate Sample Data

```python
# For testing pipelines without real data
import pandas as pd
import numpy as np

# Generate sample bike availability data
dates = pd.date_range('2024-01-01', periods=1000, freq='H')
sample_data = pd.DataFrame({
    'timestamp': dates,
    'station_id': np.random.choice(['S001', 'S002', 'S003'], 1000),
    'bikes_available': np.random.randint(0, 20, 1000),
    'temperature': np.random.uniform(0, 25, 1000)
})
```

---

## ğŸ› ï¸ Data Pipeline Flow

```
1. ACQUIRE (Module 02)
   â”œâ”€â”€ APIs, downloads, web scraping
   â”œâ”€â”€ Save to raw/
   â””â”€â”€ Document sources

2. EXPLORE (Module 03)
   â”œâ”€â”€ Load from raw/
   â”œâ”€â”€ Profiling and EDA
   â””â”€â”€ Identify quality issues

3. CLEAN (Module 04)
   â”œâ”€â”€ Handle missing values
   â”œâ”€â”€ Remove duplicates
   â”œâ”€â”€ Fix data types
   â””â”€â”€ Save to processed/cleaned_*.parquet

4. ENGINEER (Module 04)
   â”œâ”€â”€ Create features
   â”œâ”€â”€ Encode categories
   â”œâ”€â”€ Scale/normalize
   â””â”€â”€ Save to processed/featured_*.parquet

5. MODEL (Module 05)
   â”œâ”€â”€ Train/test splits
   â”œâ”€â”€ Model training
   â””â”€â”€ Save predictions to processed/
```

---

## ğŸ“š Code Snippets for Data Operations

### Loading Data

```python
import pandas as pd

# Load raw CSV
df = pd.read_csv('data/raw/amsterdam_bike_api_2024-01-15.csv')

# Load processed parquet (faster)
df = pd.read_parquet('data/processed/cleaned_bike_weather_v1.parquet')

# Load JSON
df = pd.read_json('data/raw/openchargemap_stations_2024-01-15.json')
```

### Saving Data

```python
# Save to parquet (recommended for processed data)
df.to_parquet('data/processed/cleaned_bike_weather_v1.parquet', index=False)

# Save to CSV (for raw data)
df.to_csv('data/raw/amsterdam_bike_api_2024-01-15.csv', index=False)
```

**More examples**: [docs/code_snippets.md](../docs/code_snippets.md)

---

## ğŸ§ª Data Validation

Before using data in models, always validate:

```python
# Check for issues
print(f"Shape: {df.shape}")
print(f"Missing: {df.isnull().sum()}")
print(f"Duplicates: {df.duplicated().sum()}")
print(f"Data types:\n{df.dtypes}")
```

**Tools**:
- `pandas-profiling` (now `ydata-profiling`) - Automated reports
- `great_expectations` - Data validation pipelines

---

## ğŸ“¦ Dependencies for Data Work

Make sure you have these installed:

```bash
pip install pandas numpy scipy ydata-profiling
```

**See**: [docs/dependency_management.md](../docs/dependency_management.md) for complete setup

---

## ğŸ“– Additional Resources

### Documentation
- ğŸ“ [Coding Standards](../docs/coding_standards.md) - Data handling best practices
- ğŸ“š [Code Snippets](../docs/code_snippets.md) - Quick data operations
- ğŸŒ [Open Data Sources](../docs/open_data_sources.md) - Where to get data

### Module Guides
- ğŸ““ [Module 02: Data Acquisition](../notebooks/Module_02_Data_Acquisition/) - API usage
- ğŸ““ [Module 03: Exploration](../notebooks/Module_03_Exploration_Profiling/) - EDA techniques
- ğŸ““ [Module 04: Feature Engineering](../notebooks/Module_04_Feature_Engineering/) - Transformations

---

## âœ… Data Quality Checklist

Before moving data to processed/:

- [ ] Source documented (where did it come from?)
- [ ] Date stamp included
- [ ] No personal/sensitive information
- [ ] Reasonable file size (< 100MB or compressed)
- [ ] Missing values handled
- [ ] Data types correct
- [ ] Duplicates removed
- [ ] Transformations documented

---

## ğŸ†˜ Common Issues

### Issue: File too large for Git
**Solution**: Add to `.gitignore` and document how to obtain it

### Issue: Missing data files
**Solution**: Run data acquisition notebooks in Module 02

### Issue: Import errors
**Solution**: Check [dependency_management.md](../docs/dependency_management.md)

### Issue: Can't find data files
**Solution**: Use relative paths from project root:
```python
import os
data_path = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
```

---

**Next steps**: Start with [Module 02](../notebooks/Module_02_Data_Acquisition/) to acquire your first dataset! ğŸš€
